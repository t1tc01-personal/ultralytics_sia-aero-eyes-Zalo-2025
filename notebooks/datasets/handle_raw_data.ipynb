{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a42315ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -qqq tifffile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b850d919",
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_ROOT = '/home/t1tc01-hoangphan/code/t1tc01-personal/ultralytics_sia-aero-eyes-Zalo-2025/dataset_output'\n",
    "DATASET_ROOT= '/home/t1tc01-hoangphan/code/t1tc01-personal/Zalo-AI-2025-Challenger/aero-eyes-data/train'\n",
    "class_names = ['Backpack', \n",
    "'Jacket', \n",
    "'Laptop', \n",
    "'Lifering', \n",
    "'Mobilephone', \n",
    "'Person1', \n",
    "'WaterBottle'\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e3ca6d4",
   "metadata": {},
   "source": [
    "## Analyze data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "145def5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "üìä PH√ÇN T√çCH DATASET STATISTICS\n",
      "======================================================================\n",
      "\n",
      "üìÅ T·ªïng s·ªë videos: 14\n",
      "   - Train videos: 11 (80%)\n",
      "   - Val videos: 3 (20%)\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "üìπ Chi ti·∫øt t·ª´ng video:\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "[1/14] Backpack_0 (train)\n",
      "   Class: Backpack\n",
      "   Duration: 418.6s (7.0 min)\n",
      "   Total frames: 10,466 @ 25.0 fps\n",
      "   Annotated frames: 3,184 (30.4%)\n",
      "   Bboxes: 3,189\n",
      "   Negative samples: 955\n",
      "   Total samples: 4,139\n",
      "\n",
      "[2/14] Backpack_1 (train)\n",
      "   Class: Backpack\n",
      "   Duration: 180.0s (3.0 min)\n",
      "   Total frames: 4,500 @ 25.0 fps\n",
      "   Annotated frames: 1,454 (32.3%)\n",
      "   Bboxes: 1,456\n",
      "   Negative samples: 436\n",
      "   Total samples: 1,890\n",
      "\n",
      "[3/14] Jacket_0 (train)\n",
      "   Class: Jacket\n",
      "   Duration: 203.4s (3.4 min)\n",
      "   Total frames: 5,085 @ 25.0 fps\n",
      "   Annotated frames: 1,162 (22.9%)\n",
      "   Bboxes: 1,165\n",
      "   Negative samples: 348\n",
      "   Total samples: 1,510\n",
      "\n",
      "[4/14] Jacket_1 (train)\n",
      "   Class: Jacket\n",
      "   Duration: 208.8s (3.5 min)\n",
      "   Total frames: 5,221 @ 25.0 fps\n",
      "   Annotated frames: 690 (13.2%)\n",
      "   Bboxes: 693\n",
      "   Negative samples: 207\n",
      "   Total samples: 897\n",
      "\n",
      "[5/14] Laptop_0 (train)\n",
      "   Class: Laptop\n",
      "   Duration: 205.7s (3.4 min)\n",
      "   Total frames: 5,142 @ 25.0 fps\n",
      "   Annotated frames: 884 (17.2%)\n",
      "   Bboxes: 889\n",
      "   Negative samples: 265\n",
      "   Total samples: 1,149\n",
      "\n",
      "[6/14] Laptop_1 (train)\n",
      "   Class: Laptop\n",
      "   Duration: 206.5s (3.4 min)\n",
      "   Total frames: 5,163 @ 25.0 fps\n",
      "   Annotated frames: 987 (19.1%)\n",
      "   Bboxes: 989\n",
      "   Negative samples: 296\n",
      "   Total samples: 1,283\n",
      "\n",
      "[7/14] Lifering_0 (train)\n",
      "   Class: Lifering\n",
      "   Duration: 187.9s (3.1 min)\n",
      "   Total frames: 4,697 @ 25.0 fps\n",
      "   Annotated frames: 1,134 (24.1%)\n",
      "   Bboxes: 1,136\n",
      "   Negative samples: 340\n",
      "   Total samples: 1,474\n",
      "\n",
      "[8/14] Lifering_1 (train)\n",
      "   Class: Lifering\n",
      "   Duration: 267.0s (4.5 min)\n",
      "   Total frames: 6,675 @ 25.0 fps\n",
      "   Annotated frames: 1,511 (22.6%)\n",
      "   Bboxes: 1,516\n",
      "   Negative samples: 453\n",
      "   Total samples: 1,964\n",
      "\n",
      "[9/14] MobilePhone_0 (train)\n",
      "   Class: MobilePhone\n",
      "   Duration: 256.4s (4.3 min)\n",
      "   Total frames: 6,410 @ 25.0 fps\n",
      "   Annotated frames: 968 (15.1%)\n",
      "   Bboxes: 985\n",
      "   Negative samples: 290\n",
      "   Total samples: 1,258\n",
      "\n",
      "[10/14] MobilePhone_1 (train)\n",
      "   Class: MobilePhone\n",
      "   Duration: 195.4s (3.3 min)\n",
      "   Total frames: 4,886 @ 25.0 fps\n",
      "   Annotated frames: 889 (18.2%)\n",
      "   Bboxes: 893\n",
      "   Negative samples: 266\n",
      "   Total samples: 1,155\n",
      "\n",
      "[11/14] Person1_0 (train)\n",
      "   Class: Person1\n",
      "   Duration: 224.4s (3.7 min)\n",
      "   Total frames: 5,609 @ 25.0 fps\n",
      "   Annotated frames: 2,057 (36.7%)\n",
      "   Bboxes: 2,064\n",
      "   Negative samples: 617\n",
      "   Total samples: 2,674\n",
      "\n",
      "[12/14] Person1_1 (val)\n",
      "   Class: Person1\n",
      "   Duration: 204.1s (3.4 min)\n",
      "   Total frames: 5,103 @ 25.0 fps\n",
      "   Annotated frames: 1,129 (22.1%)\n",
      "   Bboxes: 1,134\n",
      "   Negative samples: 338\n",
      "   Total samples: 1,467\n",
      "\n",
      "[13/14] WaterBottle_0 (val)\n",
      "   Class: WaterBottle\n",
      "   Duration: 199.9s (3.3 min)\n",
      "   Total frames: 4,998 @ 25.0 fps\n",
      "   Annotated frames: 934 (18.7%)\n",
      "   Bboxes: 938\n",
      "   Negative samples: 280\n",
      "   Total samples: 1,214\n",
      "\n",
      "[14/14] WaterBottle_1 (val)\n",
      "   Class: WaterBottle\n",
      "   Duration: 271.0s (4.5 min)\n",
      "   Total frames: 6,774 @ 25.0 fps\n",
      "   Annotated frames: 3,123 (46.1%)\n",
      "   Bboxes: 3,169\n",
      "   Negative samples: 936\n",
      "   Total samples: 4,059\n",
      "\n",
      "======================================================================\n",
      "üìà T·ªîNG K·∫æT\n",
      "======================================================================\n",
      "\n",
      "üé¨ Videos:\n",
      "   - T·ªïng videos: 14\n",
      "   - Train: 11 videos\n",
      "   - Val: 3 videos\n",
      "\n",
      "üéûÔ∏è  Frames:\n",
      "   - T·ªïng frames: 80,729\n",
      "   - Frames c√≥ annotation: 20,106 (24.9%)\n",
      "   - T·ªïng bboxes: 20,216\n",
      "   - Trung b√¨nh bboxes/frame: 1.01\n",
      "\n",
      "üì¶ Samples sau conversion:\n",
      "   Train:\n",
      "      - Positive: 14,920 samples\n",
      "      - Negative: 4,473 samples\n",
      "      - T·ªïng: 19,393 samples\n",
      "   Val:\n",
      "      - Positive: 5,186 samples\n",
      "      - Negative: 1,554 samples\n",
      "      - T·ªïng: 6,740 samples\n",
      "   T·ªîNG C·ªòNG: 26,133 samples\n",
      "\n",
      "üìä Ph√¢n b·ªë theo class:\n",
      "   - Backpack: 4,645 bboxes\n",
      "   - Jacket: 1,858 bboxes\n",
      "   - Laptop: 1,878 bboxes\n",
      "   - Lifering: 2,652 bboxes\n",
      "   - MobilePhone: 1,878 bboxes\n",
      "   - Person1: 3,198 bboxes\n",
      "   - WaterBottle: 4,107 bboxes\n",
      "\n",
      "üíæ Storage ∆∞·ªõc t√≠nh:\n",
      "   - M·ªói sample: ~8 MB\n",
      "   - T·ªïng storage: ~204.2 GB\n",
      "\n",
      "‚öôÔ∏è  Tham s·ªë conversion:\n",
      "   - train_split: 0.8\n",
      "   - frame_stride: 1\n",
      "   - negative_sample_ratio: 0.3\n",
      "\n",
      "üí° Khuy·∫øn ngh·ªã:\n",
      "   ‚úÖ Dataset v·ª´a ph·∫£i - frame_stride=1 l√† ph√π h·ª£p\n",
      "   ‚ö†Ô∏è  Storage l·ªõn (>200GB) - c√¢n nh·∫Øc tƒÉng frame_stride ho·∫∑c gi·∫£m resolution\n",
      "\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import cv2\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm\n",
    "from tifffile import imwrite\n",
    "\n",
    "def analyze_dataset_statistics(\n",
    "    dataset_root,\n",
    "    train_split=0.8,\n",
    "    frame_stride=1,\n",
    "    negative_sample_ratio=0.3\n",
    "):\n",
    "    \"\"\"\n",
    "    Ph√¢n t√≠ch v√† t√≠nh to√°n s·ªë l∆∞·ª£ng samples sau conversion\n",
    "    \n",
    "    Args:\n",
    "        dataset_root: Path to dataset root\n",
    "        train_split: Train/val split ratio\n",
    "        frame_stride: Frame sampling stride\n",
    "        negative_sample_ratio: Ratio of negative samples\n",
    "    \"\"\"\n",
    "    dataset_root = Path(dataset_root)\n",
    "    \n",
    "    # Load annotations\n",
    "    annotations_path = dataset_root / 'annotations' / 'annotations.json'\n",
    "    if not annotations_path.exists():\n",
    "        print(f\"‚ùå Kh√¥ng t√¨m th·∫•y file: {annotations_path}\")\n",
    "        return\n",
    "    \n",
    "    with open(annotations_path, 'r') as f:\n",
    "        annotations = json.load(f)\n",
    "    \n",
    "    print(\"=\" * 70)\n",
    "    print(\"üìä PH√ÇN T√çCH DATASET STATISTICS\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    # Statistics\n",
    "    total_videos = len(annotations)\n",
    "    num_train_videos = int(total_videos * train_split)\n",
    "    num_val_videos = total_videos - num_train_videos\n",
    "    \n",
    "    video_stats = []\n",
    "    total_frames = 0\n",
    "    total_annotated_frames = 0\n",
    "    total_bboxes = 0\n",
    "    class_counts = defaultdict(int)\n",
    "    \n",
    "    print(f\"\\nüìÅ T·ªïng s·ªë videos: {total_videos}\")\n",
    "    print(f\"   - Train videos: {num_train_videos} ({train_split*100:.0f}%)\")\n",
    "    print(f\"   - Val videos: {num_val_videos} ({(1-train_split)*100:.0f}%)\")\n",
    "    print(\"\\n\" + \"-\" * 70)\n",
    "    print(\"üìπ Chi ti·∫øt t·ª´ng video:\")\n",
    "    print(\"-\" * 70)\n",
    "    \n",
    "    for idx, video_data in enumerate(annotations):\n",
    "        video_id = video_data['video_id']\n",
    "        video_path = dataset_root / 'samples' / video_id / 'drone_video.mp4'\n",
    "        \n",
    "        # Get class\n",
    "        class_name = video_id.rsplit('_', 1)[0]\n",
    "        \n",
    "        # Count annotations\n",
    "        annotated_frames = set()\n",
    "        bbox_count = 0\n",
    "        for ann in video_data['annotations']:\n",
    "            for bbox in ann['bboxes']:\n",
    "                frame_num = bbox['frame']\n",
    "                annotated_frames.add(frame_num)\n",
    "                bbox_count += 1\n",
    "                class_counts[class_name] += 1\n",
    "        \n",
    "        num_annotated_frames = len(annotated_frames)\n",
    "        \n",
    "        # Get video info\n",
    "        total_video_frames = 0\n",
    "        fps = 30  # default\n",
    "        duration = 0\n",
    "        if video_path.exists():\n",
    "            cap = cv2.VideoCapture(str(video_path))\n",
    "            total_video_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "            fps = cap.get(cv2.CAP_PROP_FPS) or 30\n",
    "            duration = total_video_frames / fps\n",
    "            cap.release()\n",
    "        else:\n",
    "            print(f\"‚ö†Ô∏è  Video kh√¥ng t·ªìn t·∫°i: {video_path}\")\n",
    "        \n",
    "        # Calculate negative candidates\n",
    "        negative_candidates = []\n",
    "        for frame_num in range(0, total_video_frames, frame_stride):\n",
    "            if frame_num not in annotated_frames:\n",
    "                negative_candidates.append(frame_num)\n",
    "        \n",
    "        num_negative = min(\n",
    "            int(num_annotated_frames * negative_sample_ratio),\n",
    "            len(negative_candidates)\n",
    "        )\n",
    "        \n",
    "        split = 'train' if idx < num_train_videos else 'val'\n",
    "        \n",
    "        video_stats.append({\n",
    "            'video_id': video_id,\n",
    "            'class': class_name,\n",
    "            'split': split,\n",
    "            'total_frames': total_video_frames,\n",
    "            'annotated_frames': num_annotated_frames,\n",
    "            'bboxes': bbox_count,\n",
    "            'negative_samples': num_negative,\n",
    "            'duration': duration,\n",
    "            'fps': fps\n",
    "        })\n",
    "        \n",
    "        total_frames += total_video_frames\n",
    "        total_annotated_frames += num_annotated_frames\n",
    "        total_bboxes += bbox_count\n",
    "        \n",
    "        # Print video info\n",
    "        print(f\"\\n[{idx+1}/{total_videos}] {video_id} ({split})\")\n",
    "        print(f\"   Class: {class_name}\")\n",
    "        print(f\"   Duration: {duration:.1f}s ({duration/60:.1f} min)\")\n",
    "        print(f\"   Total frames: {total_video_frames:,} @ {fps:.1f} fps\")\n",
    "        print(f\"   Annotated frames: {num_annotated_frames:,} ({num_annotated_frames/total_video_frames*100:.1f}%)\")\n",
    "        print(f\"   Bboxes: {bbox_count:,}\")\n",
    "        print(f\"   Negative samples: {num_negative:,}\")\n",
    "        print(f\"   Total samples: {num_annotated_frames + num_negative:,}\")\n",
    "    \n",
    "    # Calculate totals by split\n",
    "    train_positive = sum(s['annotated_frames'] for s in video_stats if s['split'] == 'train')\n",
    "    train_negative = sum(s['negative_samples'] for s in video_stats if s['split'] == 'train')\n",
    "    val_positive = sum(s['annotated_frames'] for s in video_stats if s['split'] == 'val')\n",
    "    val_negative = sum(s['negative_samples'] for s in video_stats if s['split'] == 'val')\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"üìà T·ªîNG K·∫æT\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    print(f\"\\nüé¨ Videos:\")\n",
    "    print(f\"   - T·ªïng videos: {total_videos}\")\n",
    "    print(f\"   - Train: {num_train_videos} videos\")\n",
    "    print(f\"   - Val: {num_val_videos} videos\")\n",
    "    \n",
    "    print(f\"\\nüéûÔ∏è  Frames:\")\n",
    "    print(f\"   - T·ªïng frames: {total_frames:,}\")\n",
    "    print(f\"   - Frames c√≥ annotation: {total_annotated_frames:,} ({total_annotated_frames/total_frames*100:.1f}%)\")\n",
    "    print(f\"   - T·ªïng bboxes: {total_bboxes:,}\")\n",
    "    print(f\"   - Trung b√¨nh bboxes/frame: {total_bboxes/total_annotated_frames:.2f}\")\n",
    "    \n",
    "    print(f\"\\nüì¶ Samples sau conversion:\")\n",
    "    print(f\"   Train:\")\n",
    "    print(f\"      - Positive: {train_positive:,} samples\")\n",
    "    print(f\"      - Negative: {train_negative:,} samples\")\n",
    "    print(f\"      - T·ªïng: {train_positive + train_negative:,} samples\")\n",
    "    print(f\"   Val:\")\n",
    "    print(f\"      - Positive: {val_positive:,} samples\")\n",
    "    print(f\"      - Negative: {val_negative:,} samples\")\n",
    "    print(f\"      - T·ªïng: {val_positive + val_negative:,} samples\")\n",
    "    print(f\"   T·ªîNG C·ªòNG: {train_positive + train_negative + val_positive + val_negative:,} samples\")\n",
    "    \n",
    "    print(f\"\\nüìä Ph√¢n b·ªë theo class:\")\n",
    "    for class_name, count in sorted(class_counts.items()):\n",
    "        print(f\"   - {class_name}: {count:,} bboxes\")\n",
    "    \n",
    "    # Storage estimation\n",
    "    avg_size_per_sample = 8  # MB (6-channel TIFF)\n",
    "    total_storage = (train_positive + train_negative + val_positive + val_negative) * avg_size_per_sample / 1024\n",
    "    \n",
    "    print(f\"\\nüíæ Storage ∆∞·ªõc t√≠nh:\")\n",
    "    print(f\"   - M·ªói sample: ~{avg_size_per_sample} MB\")\n",
    "    print(f\"   - T·ªïng storage: ~{total_storage:.1f} GB\")\n",
    "    \n",
    "    # Frame stride impact\n",
    "    print(f\"\\n‚öôÔ∏è  Tham s·ªë conversion:\")\n",
    "    print(f\"   - train_split: {train_split}\")\n",
    "    print(f\"   - frame_stride: {frame_stride}\")\n",
    "    print(f\"   - negative_sample_ratio: {negative_sample_ratio}\")\n",
    "    \n",
    "    # Recommendations\n",
    "    print(f\"\\nüí° Khuy·∫øn ngh·ªã:\")\n",
    "    if total_annotated_frames < 10000:\n",
    "        print(f\"   ‚ö†Ô∏è  Dataset nh·ªè (<10K samples) - n√™n d√πng frame_stride=1 ƒë·ªÉ t·ªëi ƒëa d·ªØ li·ªáu\")\n",
    "    elif total_annotated_frames > 50000:\n",
    "        print(f\"   üí° Dataset l·ªõn (>50K samples) - c√≥ th·ªÉ d√πng frame_stride=2-3 ƒë·ªÉ gi·∫£m storage\")\n",
    "    else:\n",
    "        print(f\"   ‚úÖ Dataset v·ª´a ph·∫£i - frame_stride=1 l√† ph√π h·ª£p\")\n",
    "    \n",
    "    if total_storage > 200:\n",
    "        print(f\"   ‚ö†Ô∏è  Storage l·ªõn (>200GB) - c√¢n nh·∫Øc tƒÉng frame_stride ho·∫∑c gi·∫£m resolution\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    \n",
    "    return {\n",
    "        'total_videos': total_videos,\n",
    "        'total_frames': total_frames,\n",
    "        'total_annotated_frames': total_annotated_frames,\n",
    "        'total_bboxes': total_bboxes,\n",
    "        'train_positive': train_positive,\n",
    "        'train_negative': train_negative,\n",
    "        'val_positive': val_positive,\n",
    "        'val_negative': val_negative,\n",
    "        'total_samples': train_positive + train_negative + val_positive + val_negative,\n",
    "        'estimated_storage_gb': total_storage,\n",
    "        'video_stats': video_stats\n",
    "    }\n",
    "\n",
    "# Ch·∫°y ph√¢n t√≠ch\n",
    "stats = analyze_dataset_statistics(\n",
    "    dataset_root=DATASET_ROOT,\n",
    "    train_split=0.8,\n",
    "    frame_stride=1,\n",
    "    negative_sample_ratio=0.3\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7433e59",
   "metadata": {},
   "source": [
    "## Conversion Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3afde9c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import cv2\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from tifffile import imwrite\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d596fc23",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_class_id(class_name):\n",
    "    \"\"\"Map class name to class ID\"\"\"\n",
    "    class_mapping = {\n",
    "        'Backpack': 0,\n",
    "        'Jacket': 1,\n",
    "        'Laptop': 2,\n",
    "        'Lifering': 3,\n",
    "        'MobilePhone': 4,\n",
    "        'Person1': 5,\n",
    "        'WaterBottle': 6,\n",
    "    }\n",
    "    return class_mapping.get(class_name, 0)\n",
    "\n",
    "def create_data_yaml(output_root, class_names):\n",
    "    \"\"\"Create data.yaml file\"\"\"\n",
    "    output_root = Path(output_root)\n",
    "    yaml_content = f\"\"\"names:\n",
    "{chr(10).join([f\"- {name}\" for name in class_names])}\n",
    "nc: {len(class_names)}\n",
    "\n",
    "train: train/images\n",
    "val: val/images\n",
    "test: test/images\n",
    "\"\"\"\n",
    "    with open(output_root / 'data.yaml', 'w') as f:\n",
    "        f.write(yaml_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "04215fb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_video_to_siamese_format(\n",
    "    dataset_root,\n",
    "    output_root,\n",
    "    reference_frame_offset=10,  # Use frame N frames before as reference\n",
    "    frame_stride=1  # Sample every Nth frame\n",
    "):\n",
    "    \"\"\"\n",
    "    Convert video dataset to siamese YOLO format.\n",
    "    \n",
    "    Strategy: Use previous frame as \"clean\" reference, current frame as \"annotated\"\n",
    "    \"\"\"\n",
    "    dataset_root = Path(dataset_root)\n",
    "    output_root = Path(output_root)\n",
    "    \n",
    "    # Load annotations\n",
    "    with open(dataset_root / 'annotations' / 'annotations.json', 'r') as f:\n",
    "        annotations = json.load(f)\n",
    "    \n",
    "    # Create output directories\n",
    "    for split in ['train', 'val']:\n",
    "        (output_root / split / 'images').mkdir(parents=True, exist_ok=True)\n",
    "        (output_root / split / 'labels').mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # Process each video\n",
    "    for video_data in tqdm(annotations, desc=\"Processing videos\"):\n",
    "        video_id = video_data['video_id']\n",
    "        video_path = dataset_root / 'samples' / video_id / 'drone_video.mp4'\n",
    "        \n",
    "        if not video_path.exists():\n",
    "            continue\n",
    "        \n",
    "        # Extract class from video_id (e.g., \"Backpack_0\" -> \"Backpack\")\n",
    "        class_name = video_id.rsplit('_', 1)[0]\n",
    "        class_id = get_class_id(class_name)  # You'll need to define this\n",
    "        \n",
    "        # Load video\n",
    "        cap = cv2.VideoCapture(str(video_path))\n",
    "        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "        \n",
    "        # Create frame-to-bbox mapping\n",
    "        bbox_map = {}\n",
    "        for ann in video_data['annotations']:\n",
    "            for bbox in ann['bboxes']:\n",
    "                frame_num = bbox['frame']\n",
    "                if frame_num not in bbox_map:\n",
    "                    bbox_map[frame_num] = []\n",
    "                bbox_map[frame_num].append({\n",
    "                    'x1': bbox['x1'],\n",
    "                    'y1': bbox['y1'],\n",
    "                    'x2': bbox['x2'],\n",
    "                    'y2': bbox['y2']\n",
    "                })\n",
    "        \n",
    "        # Process frames\n",
    "        frame_buffer = []\n",
    "        frame_count = 0\n",
    "        \n",
    "        while True:\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "            \n",
    "            frame_buffer.append(frame)\n",
    "            \n",
    "            # When we have enough frames, create siamese pairs\n",
    "            if len(frame_buffer) > reference_frame_offset:\n",
    "                # Reference frame (clean)\n",
    "                ref_frame = frame_buffer[-reference_frame_offset]\n",
    "                # Current frame (with annotations)\n",
    "                curr_frame = frame_buffer[-1]\n",
    "                \n",
    "                # Create 6-channel image: [ref_R, ref_G, ref_B, curr_R, curr_G, curr_B]\n",
    "                ref_rgb = cv2.cvtColor(ref_frame, cv2.COLOR_BGR2RGB)\n",
    "                curr_rgb = cv2.cvtColor(curr_frame, cv2.COLOR_BGR2RGB)\n",
    "                six_channel = np.concatenate([ref_rgb, curr_rgb], axis=2)\n",
    "                \n",
    "                # Get image dimensions\n",
    "                h, w = curr_frame.shape[:2]\n",
    "                \n",
    "                # Create label file\n",
    "                label_lines = []\n",
    "                if frame_count in bbox_map:\n",
    "                    for bbox in bbox_map[frame_count]:\n",
    "                        # Convert xyxy to normalized xywh\n",
    "                        x1, y1, x2, y2 = bbox['x1'], bbox['y1'], bbox['x2'], bbox['y2']\n",
    "                        \n",
    "                        # Normalize\n",
    "                        x_center = ((x1 + x2) / 2) / w\n",
    "                        y_center = ((y1 + y2) / 2) / h\n",
    "                        width = (x2 - x1) / w\n",
    "                        height = (y2 - y1) / h\n",
    "                        \n",
    "                        label_lines.append(f\"{class_id} {x_center} {y_center} {width} {height}\")\n",
    "                \n",
    "                # Save 6-channel TIFF\n",
    "                image_name = f\"{video_id}_frame_{frame_count:06d}\"\n",
    "                image_path = output_root / 'train' / 'images' / f\"{image_name}.tif\"\n",
    "                imwrite(str(image_path), six_channel)\n",
    "                \n",
    "                # Save label\n",
    "                label_path = output_root / 'train' / 'labels' / f\"{image_name}.txt\"\n",
    "                with open(label_path, 'w') as f:\n",
    "                    f.write('\\n'.join(label_lines))\n",
    "                \n",
    "                frame_count += frame_stride\n",
    "                \n",
    "                # Keep buffer size manageable\n",
    "                if len(frame_buffer) > reference_frame_offset + 10:\n",
    "                    frame_buffer.pop(0)\n",
    "        \n",
    "        cap.release()\n",
    "    \n",
    "    # Create data.yaml\n",
    "    create_data_yaml(output_root, class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0359485d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_with_object_templates(\n",
    "    dataset_root, \n",
    "    output_root, \n",
    "    train_split=0.8, \n",
    "    frame_stride=2,  # ‚≠ê Best option: process every 2nd frame\n",
    "    negative_sample_ratio=0.25,  # ‚≠ê Best option: 25% negative samples\n",
    "    target_size=(640, 640),  # ‚≠ê Best option: resize to 640x640\n",
    "    compress_level=6  # ‚≠ê Best option: compression level 6\n",
    "):\n",
    "    \"\"\"\n",
    "    Use object_images as reference, video frames as query\n",
    "    \n",
    "    Best option settings:\n",
    "    - Resolution: 640x640 (reduces storage by ~80%)\n",
    "    - Frame stride: 2 (process every 2nd frame)\n",
    "    - Negative ratio: 25% (balanced dataset)\n",
    "    - Compression: level 6 (reduces storage by ~40%)\n",
    "    \"\"\"\n",
    "    dataset_root = Path(dataset_root)\n",
    "    output_root = Path(output_root)\n",
    "    \n",
    "    # Load annotations\n",
    "    with open(dataset_root / 'annotations' / 'annotations.json', 'r') as f:\n",
    "        annotations = json.load(f)\n",
    "    \n",
    "    # Create output directories\n",
    "    for split in ['train', 'val']:\n",
    "        (output_root / split / 'images').mkdir(parents=True, exist_ok=True)\n",
    "        (output_root / split / 'labels').mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # Determine train/val split by video\n",
    "    num_videos = len(annotations)\n",
    "    train_count = int(num_videos * train_split)\n",
    "    \n",
    "    # Compression mapping for tifffile\n",
    "    # tifffile uses 'compression' parameter: None, 'lzw', 'zlib', 'jpeg', etc.\n",
    "    compression_map = {\n",
    "        1: 'lzw',      # LZW compression (good balance)\n",
    "        2: 'lzw',\n",
    "        3: 'lzw',\n",
    "        4: 'zlib',     # Zlib compression (better compression)\n",
    "        5: 'zlib',\n",
    "        6: 'zlib',     # Best option default\n",
    "        7: 'zlib',\n",
    "        8: 'zlib',\n",
    "        9: 'zlib'\n",
    "    }\n",
    "    compression_method = compression_map.get(compress_level, 'zlib') if compress_level > 0 else None\n",
    "    \n",
    "    # Collect all frames with annotations for negative sampling\n",
    "    all_annotated_frames = set()\n",
    "    for video_data in annotations:\n",
    "        for ann in video_data['annotations']:\n",
    "            for bbox in ann['bboxes']:\n",
    "                all_annotated_frames.add((video_data['video_id'], bbox['frame']))\n",
    "    \n",
    "    # Process each video\n",
    "    for idx, video_data in enumerate(tqdm(annotations, desc=\"Processing videos\")):\n",
    "        video_id = video_data['video_id']\n",
    "        video_path = dataset_root / 'samples' / video_id / 'drone_video.mp4'\n",
    "        object_images_dir = dataset_root / 'samples' / video_id / 'object_images'\n",
    "        \n",
    "        if not video_path.exists():\n",
    "            continue\n",
    "        \n",
    "        split = 'train' if idx < train_count else 'val'\n",
    "        class_name = video_id.rsplit('_', 1)[0]\n",
    "        class_id = get_class_id(class_name)\n",
    "        \n",
    "        # Load template images\n",
    "        template_images = sorted(list(object_images_dir.glob('*.jpg')))\n",
    "        if not template_images:\n",
    "            continue\n",
    "        \n",
    "        ref_images = []\n",
    "        for template_path in template_images[:3]:\n",
    "            img = cv2.imread(str(template_path))\n",
    "            if img is not None:\n",
    "                ref_images.append(img)\n",
    "        \n",
    "        if not ref_images:\n",
    "            continue\n",
    "        \n",
    "        # Resize all template images to the same size (use first image's size as reference)\n",
    "        target_h, target_w = ref_images[0].shape[:2]\n",
    "        ref_images_resized = []\n",
    "        for img in ref_images:\n",
    "            if img.shape[:2] != (target_h, target_w):\n",
    "                img_resized = cv2.resize(img, (target_w, target_h), interpolation=cv2.INTER_LINEAR)\n",
    "                ref_images_resized.append(img_resized)\n",
    "            else:\n",
    "                ref_images_resized.append(img)\n",
    "        \n",
    "        # ‚≠ê Median + Post-processing (khuy·∫øn ngh·ªã)\n",
    "        # 1. D√πng median thay v√¨ mean (√≠t m·ªù h∆°n, gi·ªØ m√†u s·∫Øc t·ªët h∆°n)\n",
    "        ref_images_array = np.array(ref_images_resized)\n",
    "        ref_image = np.median(ref_images_array, axis=0).astype(np.uint8)\n",
    "        \n",
    "        # 2. TƒÉng saturation ƒë·ªÉ l√†m r√µ m√†u s·∫Øc v·∫≠t th·ªÉ\n",
    "        hsv = cv2.cvtColor(ref_image, cv2.COLOR_BGR2HSV).astype(np.float32)\n",
    "        hsv[:, :, 1] = np.clip(hsv[:, :, 1] * 1.15, 0, 255)  # TƒÉng saturation 15%\n",
    "        hsv[:, :, 2] = np.clip(hsv[:, :, 2] * 1.05, 0, 255)  # TƒÉng brightness 5%\n",
    "        ref_image = cv2.cvtColor(hsv.astype(np.uint8), cv2.COLOR_HSV2BGR)\n",
    "        \n",
    "        # 3. TƒÉng contrast nh·∫π\n",
    "        ref_image = cv2.convertScaleAbs(ref_image, alpha=1.1, beta=5)\n",
    "        \n",
    "        # Convert to RGB\n",
    "        ref_rgb = cv2.cvtColor(ref_image, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        # Create frame-to-bbox mapping\n",
    "        bbox_map = {}\n",
    "        for ann in video_data['annotations']:\n",
    "            for bbox in ann['bboxes']:\n",
    "                frame_num = bbox['frame']\n",
    "                if frame_num not in bbox_map:\n",
    "                    bbox_map[frame_num] = []\n",
    "                bbox_map[frame_num].append({\n",
    "                    'x1': bbox['x1'], 'y1': bbox['y1'],\n",
    "                    'x2': bbox['x2'], 'y2': bbox['y2']\n",
    "                })\n",
    "        \n",
    "        # Get video info\n",
    "        cap = cv2.VideoCapture(str(video_path))\n",
    "        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "        cap.release()\n",
    "        \n",
    "        # Collect frames for negative sampling\n",
    "        positive_frames = set(bbox_map.keys())\n",
    "        negative_candidates = []\n",
    "        for frame_num in range(0, total_frames, frame_stride):\n",
    "            if frame_num not in positive_frames:\n",
    "                negative_candidates.append(frame_num)\n",
    "        \n",
    "        # Calculate how many negative samples to include\n",
    "        num_positive = len(positive_frames)\n",
    "        num_negative = int(num_positive * negative_sample_ratio)\n",
    "        selected_negatives = np.random.choice(\n",
    "            negative_candidates, \n",
    "            min(num_negative, len(negative_candidates)), \n",
    "            replace=False\n",
    "        )\n",
    "        negative_frames = set(selected_negatives)\n",
    "        \n",
    "        # Process video frames\n",
    "        cap = cv2.VideoCapture(str(video_path))\n",
    "        frame_count = 0\n",
    "        processed_count = 0\n",
    "        \n",
    "        while True:\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "            \n",
    "            # Process positive frames (with annotations) and selected negative frames\n",
    "            is_positive = frame_count in bbox_map\n",
    "            is_negative = frame_count in negative_frames\n",
    "            \n",
    "            if not (is_positive or is_negative):\n",
    "                frame_count += 1\n",
    "                continue\n",
    "            \n",
    "            # Get original frame dimensions for bbox normalization\n",
    "            original_h, original_w = frame.shape[:2]\n",
    "            \n",
    "            # Resize reference to match frame\n",
    "            ref_resized = cv2.resize(ref_rgb, (original_w, original_h), interpolation=cv2.INTER_LINEAR)\n",
    "            \n",
    "            # Create 6-channel image\n",
    "            frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "            six_channel = np.concatenate([ref_resized, frame_rgb], axis=2)\n",
    "            \n",
    "            # ‚≠ê Resize to target size if specified\n",
    "            if target_size:\n",
    "                target_w, target_h = target_size\n",
    "                # Resize 6-channel image (resize each channel separately)\n",
    "                six_channel_resized = np.zeros((target_h, target_w, 6), dtype=six_channel.dtype)\n",
    "                for i in range(6):\n",
    "                    six_channel_resized[:, :, i] = cv2.resize(\n",
    "                        six_channel[:, :, i], \n",
    "                        (target_w, target_h), \n",
    "                        interpolation=cv2.INTER_LINEAR\n",
    "                    )\n",
    "                six_channel = six_channel_resized\n",
    "                # Update dimensions for bbox normalization\n",
    "                h, w = target_h, target_w\n",
    "                # Calculate scale factors\n",
    "                scale_x = target_w / original_w\n",
    "                scale_y = target_h / original_h\n",
    "            else:\n",
    "                h, w = original_h, original_w\n",
    "                scale_x, scale_y = 1.0, 1.0\n",
    "            \n",
    "            # Create label file\n",
    "            label_lines = []\n",
    "            if is_positive and frame_count in bbox_map:\n",
    "                for bbox in bbox_map[frame_count]:\n",
    "                    x1, y1, x2, y2 = bbox['x1'], bbox['y1'], bbox['x2'], bbox['y2']\n",
    "                    \n",
    "                    # Clamp to original image bounds\n",
    "                    x1 = max(0, min(x1, original_w))\n",
    "                    y1 = max(0, min(y1, original_h))\n",
    "                    x2 = max(0, min(x2, original_w))\n",
    "                    y2 = max(0, min(y2, original_h))\n",
    "                    \n",
    "                    if x2 <= x1 or y2 <= y1:\n",
    "                        continue\n",
    "                    \n",
    "                    # Scale bbox coordinates if image was resized\n",
    "                    if target_size:\n",
    "                        x1 = x1 * scale_x\n",
    "                        y1 = y1 * scale_y\n",
    "                        x2 = x2 * scale_x\n",
    "                        y2 = y2 * scale_y\n",
    "                    \n",
    "                    # Normalize to [0, 1]\n",
    "                    x_center = ((x1 + x2) / 2) / w\n",
    "                    y_center = ((y1 + y2) / 2) / h\n",
    "                    width = (x2 - x1) / w\n",
    "                    height = (y2 - y1) / h\n",
    "                    \n",
    "                    # Ensure normalized values are in valid range\n",
    "                    x_center = max(0.0, min(1.0, x_center))\n",
    "                    y_center = max(0.0, min(1.0, y_center))\n",
    "                    width = max(0.001, min(1.0, width))\n",
    "                    height = max(0.001, min(1.0, height))\n",
    "                    \n",
    "                    label_lines.append(f\"{class_id} {x_center:.6f} {y_center:.6f} {width:.6f} {height:.6f}\")\n",
    "            \n",
    "            # Save 6-channel TIFF with compression\n",
    "            image_name = f\"{video_id}_frame_{frame_count:06d}\"\n",
    "            image_path = output_root / split / 'images' / f\"{image_name}.tif\"\n",
    "            if compression_method:\n",
    "                imwrite(str(image_path), six_channel, compression=compression_method)\n",
    "            else:\n",
    "                imwrite(str(image_path), six_channel)\n",
    "            \n",
    "            # Save label (empty for negative samples)\n",
    "            label_path = output_root / split / 'labels' / f\"{image_name}.txt\"\n",
    "            with open(label_path, 'w') as f:\n",
    "                if label_lines:\n",
    "                    f.write('\\n'.join(label_lines))\n",
    "            \n",
    "            frame_count += 1\n",
    "            processed_count += 1\n",
    "        \n",
    "        cap.release()\n",
    "    \n",
    "    # Create data.yaml\n",
    "    create_data_yaml(output_root, class_names)\n",
    "    \n",
    "    # Calculate statistics\n",
    "    train_images = len(list((output_root / 'train' / 'images').glob('*.tif')))\n",
    "    val_images = len(list((output_root / 'val' / 'images').glob('*.tif')))\n",
    "    total_samples = train_images + val_images\n",
    "    \n",
    "    # Estimate storage\n",
    "    if target_size:\n",
    "        w, h = target_size\n",
    "        size_per_sample_mb = (w * h * 6) / (1024 * 1024)  # MB\n",
    "        if compress_level > 0:\n",
    "            size_per_sample_mb *= 0.6  # Compression reduces by ~40%\n",
    "    else:\n",
    "        size_per_sample_mb = 8  # Default estimate\n",
    "    \n",
    "    total_storage_gb = (total_samples * size_per_sample_mb) / 1024\n",
    "    \n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"‚úÖ CONVERSION COMPLETE!\")\n",
    "    print(f\"{'='*70}\")\n",
    "    print(f\"\\nüìÅ Output directory: {output_root}\")\n",
    "    print(f\"üìä Statistics:\")\n",
    "    print(f\"   - Train videos: {train_count}/{num_videos}\")\n",
    "    print(f\"   - Val videos: {num_videos - train_count}/{num_videos}\")\n",
    "    print(f\"   - Train samples: {train_images:,}\")\n",
    "    print(f\"   - Val samples: {val_images:,}\")\n",
    "    print(f\"   - Total samples: {total_samples:,}\")\n",
    "    print(f\"\\n‚öôÔ∏è  Settings used:\")\n",
    "    print(f\"   - Resolution: {target_size if target_size else 'Original'}\")\n",
    "    print(f\"   - Frame stride: {frame_stride}\")\n",
    "    print(f\"   - Negative ratio: {negative_sample_ratio*100:.0f}%\")\n",
    "    print(f\"   - Compression: Level {compress_level if compress_level > 0 else 'None'}\")\n",
    "    print(f\"\\nüíæ Storage estimate:\")\n",
    "    print(f\"   - Size per sample: ~{size_per_sample_mb:.2f} MB\")\n",
    "    print(f\"   - Total storage: ~{total_storage_gb:.1f} GB\")\n",
    "    print(f\"\\n‚úÖ Ready for training! Use: model.train(data='{output_root}/data.yaml', ...)\")\n",
    "    print(f\"{'='*70}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c9c4cf56",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing videos: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 14/14 [1:18:05<00:00, 334.69s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "‚úÖ CONVERSION COMPLETE!\n",
      "======================================================================\n",
      "\n",
      "üìÅ Output directory: /home/t1tc01-hoangphan/code/t1tc01-personal/ultralytics_sia-aero-eyes-Zalo-2025/dataset_output\n",
      "üìä Statistics:\n",
      "   - Train videos: 11/14\n",
      "   - Val videos: 3/14\n",
      "   - Train samples: 18,646\n",
      "   - Val samples: 6,481\n",
      "   - Total samples: 25,127\n",
      "\n",
      "‚öôÔ∏è  Settings used:\n",
      "   - Resolution: (640, 640)\n",
      "   - Frame stride: 2\n",
      "   - Negative ratio: 25%\n",
      "   - Compression: Level 6\n",
      "\n",
      "üíæ Storage estimate:\n",
      "   - Size per sample: ~1.41 MB\n",
      "   - Total storage: ~34.5 GB\n",
      "\n",
      "‚úÖ Ready for training! Use: model.train(data='/home/t1tc01-hoangphan/code/t1tc01-personal/ultralytics_sia-aero-eyes-Zalo-2025/dataset_output/data.yaml', ...)\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# Run conversion with BEST OPTION settings\n",
    "# Default settings (already optimized):\n",
    "# - Resolution: 640x640 (reduces storage by ~80%)\n",
    "# - Frame stride: 2 (process every 2nd frame)\n",
    "# - Negative ratio: 25% (balanced dataset)\n",
    "# - Compression: level 6 (reduces storage by ~40%)\n",
    "# Expected storage: ~16-20 GB (instead of 200+ GB)\n",
    "\n",
    "convert_with_object_templates(\n",
    "    dataset_root=DATASET_ROOT,\n",
    "    output_root=OUTPUT_ROOT,\n",
    "    train_split=0.8,  # 80% videos for train, 20% for val\n",
    "    # All other parameters use best option defaults:\n",
    "    # - frame_stride=2\n",
    "    # - negative_sample_ratio=0.25\n",
    "    # - target_size=(640, 640)\n",
    "    # - compress_level=6\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3583df88",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
